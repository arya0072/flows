id: generic_extract_load_infor_append
namespace: dev

inputs:
  - id: append_strategy
    type: SELECT
    description: "Determines the append logic. Allowed values: DAILY, MONTHLY."
    required: true
    values:
      - DAILY
      - MONTHLY
      - MONTHLY_YEAR
      - MONTHLY_SITE
      - MONTHLY_YEAR_SITE
  - id: date_field
    type: STRING
    description: "The name of the date or month_year column in BQ for the DELETE statement."
    required: true
  - id: date_year_field
    type: STRING
    description: "The name of the date or year column in BQ for the DELETE statement."
    required: false
  - id: api_path_and_query
    type: STRING
    description: "The full path and query string for the Infor API endpoint, with date placeholders."
    required: true
  - id: bq_destination_table
    type: STRING
    description: "The full BigQuery destination table name (e.g., my_dataset.my_table)."
    required: true
  - id: bq_schema_fields
    type: STRING
    required: true
    description: "A list of JSON string defining the BigQuery table schema fields."
  - id: start_date
    type: DATE
    required: true
  - id: end_date
    type: DATE
    required: false
  - id: year
    type: STRING
    required: false
  - id: month
    type: STRING
    description: "A month with 'MMM' format (e.g., MAR, JAN)."
    required: false
  - id: site
    type: STRING
    required: false
  - id: site_field
    type: STRING
    required: false

tasks:

  - id: remove_previous_data
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ inputs.append_strategy }}"
    cases:
      MONTHLY_YEAR_SITE:
        - id: delete_monthly_year_site
          type: io.kestra.plugin.gcp.bigquery.Query
          projectId: "{{ kv('GCP_PROJECT_ID') }}"
          serviceAccount: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
          sql: |
            DELETE FROM `{{ inputs.bq_destination_table }}` a
            WHERE a.{{ inputs.date_year_field }} = '{{ inputs.start_date | date('yyyy') }}' AND a.{{ inputs.date_field }} = '{{ inputs.start_date | date('MMM') | upper }}' AND {{ inputs.site_field}} = "{{ inputs.site }}"
      MONTHLY_YEAR:
        - id: delete_monthly_year_data
          type: io.kestra.plugin.gcp.bigquery.Query
          projectId: "{{ kv('GCP_PROJECT_ID') }}"
          serviceAccount: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
          sql: |
            DELETE FROM `{{ inputs.bq_destination_table }}` a
            WHERE a.{{ inputs.date_year_field }} = '{{ inputs.start_date | date('yyyy') }}' AND a.{{ inputs.date_field }} = '{{ inputs.start_date | date('MMM') | upper }}'

      MONTHLY:
        - id: delete_monthly_data
          type: io.kestra.plugin.gcp.bigquery.Query
          projectId: "{{ kv('GCP_PROJECT_ID') }}"
          serviceAccount: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
          sql: |
            DELETE FROM `{{ inputs.bq_destination_table }}` 
            WHERE {{ inputs.date_field }} = '{{ inputs.start_date | date('yyyy-MM') }}'

      MONTHLY_SITE:
        - id: delete_monthly_site_data
          type: io.kestra.plugin.gcp.bigquery.Query
          projectId: "{{ kv('GCP_PROJECT_ID') }}"
          serviceAccount: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
          sql: |
            DELETE FROM `{{ inputs.bq_destination_table }}` 
            WHERE {{ inputs.date_field }} = '{{ inputs.start_date | date('yyyy-MM') }}' AND {{ inputs.site_field}} = "{{ inputs.site }}"

      # This block runs only if append_strategy is 'DAILY'
      DAILY:
        - id: delete_daily_data
          type: io.kestra.plugin.gcp.bigquery.Query
          projectId: "{{ kv('GCP_PROJECT_ID') }}"
          serviceAccount: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
          sql: |
            DELETE FROM `{{ inputs.bq_destination_table }}` 
            WHERE {{ inputs.date_field }} BETWEEN '{{ inputs.start_date | date('yyyy-MM-dd') }}' AND '{{ inputs.end_date | date('yyyy-MM-dd') }}'
            
  - id: extract_and_load
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install pandas requests pandas-gbq google-cloud-bigquery pytz
    inputFiles:
      sa_key.json: "{{ kv('GCP_SERVICE_ACC_ETL') }}"
    script: |
      import pandas as pd
      import pandas_gbq
      import requests
      import csv
      import pytz
      import ast
      import json
      from kestra import Kestra
      from google.oauth2 import service_account

      logger = Kestra.logger()
      tags = {'source': "{{ inputs.bq_destination_table }}"}
      Kestra.outputs({"start_date": "{{ inputs.start_date }}"})
      Kestra.outputs({"end_date": "{{ inputs.end_date }}"})

      def clean_and_convert_data(df, schema_list):
        """Clean and convert DataFrame columns to match BigQuery schema."""
        logger.info("--- Starting Data Cleaning and Conversion ---")
        
        utc8 = pytz.timezone('Asia/Singapore')
        
        for col in df.columns:
          bq_type = schema_list.get(col, "STRING").upper()
          
          if bq_type == 'TIMESTAMP':
            # Handle timestamp columns
            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)
            df[col] = df[col].dt.tz_convert(utc8)
          elif bq_type == 'DATE':
            # Handle date columns
            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)
            df[col] = df[col].dt.tz_convert(utc8)
          elif bq_type == 'INT':
            # Handle integer columns
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
          elif bq_type == 'FLOAT':
            # Handle float columns
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Float64')
          elif bq_type == 'STRING':
            # Handle string columns
            df[col] = df[col].astype(str)
            df[col] = df[col].replace('nan', None)
            df[col] = df[col].replace('None', None)
          
        logger.info("Data cleaning and conversion completed.")
        return df
      
      def add_column(df: pd.DataFrame, column_name: str, column_value: str):
        if column_name not in df.columns:
          if (len(column_value) > 0): 
            df[column_name] = column_value
          else:
            df[column_name] = None
        else:
          df[column_name] = None
        logger.info(f"Added {column_name} to the df")

      def extract_and_transform(api_url, headers, strategy, start_date_str, schema_dict, date_field, site, site_field):
        """Extracts data from the API and performs basic transformation."""

        all_records = []
        bookmark = ''

        while True:
          paginated_url = api_url
          if bookmark:
            paginated_url += f"&bookmark={bookmark}"

          logger.info("--- Starting Extract & Transform Phase ---")
          logger.info(f'Requesting URL: {api_url}')
      
          api_response = requests.get(paginated_url, headers=headers)
          api_response.raise_for_status()

          records = api_response.json()['Items']
  
          if records:
            all_records.extend(records)
            logger.info(f"Fetched {len(records)} records. Total so far: {len(all_records)}.")
          
          more_rows = api_response.json()['MoreRowsExist']

          if more_rows:
            bookmark = api_response.json()['Bookmark']
            if not bookmark:
              logger.warning("API indicates more rows but returned no bookmark. Stopping loop.")
              break
          else:
            logger.info("No more rows indicated by API. Pagination complete.")
            break

        df = pd.json_normalize(all_records)
        df = clean_and_convert_data(df, schema_dict)

        if strategy == 'MONTHLY' or strategy == 'MONTHLY_SITE':
            df[date_field] = pd.to_datetime(start_date_str).strftime('%Y-%m')
        elif strategy == 'MONTHLY_YEAR' or strategy == 'MONTHLY_YEAR_SITE':
          df['Year'] = "{{ inputs.start_date | date('yyyy') }}"
          df['Month'] = "{{ inputs.start_date | date('MMM') }}".upper()
        
        if(len(site) > 0):
          #Add site column
          add_column(df, site_field, site)

        Kestra.outputs({"total_data": len(df)})
        Kestra.counter('total_data', len(df), tags)
        Kestra.outputs({"year": "{{ inputs.start_date | date('yyyy') }}"})
        Kestra.outputs({"month": "{{ inputs.start_date | date('MMM') }}".upper()})
        logger.info(f'Successfully extracted and transformed {len(df)} records.')
        return df

      def load_to_bigquery(df, project_id, destination_table, credentials):
        """Loads a pandas DataFrame to the specified BigQuery table."""
        logger.info("--- Starting Load Phase ---")
        
        if df is None or df.empty:
            logger.warning("DataFrame is empty. Nothing to load.")
            return

        logger.info(f'Loading {len(df)} records to {destination_table}.')
        pandas_gbq.to_gbq(
            dataframe=df,
            destination_table=destination_table,
            project_id=project_id, 
            if_exists='append',
            credentials=credentials
        )
        
        logger.info('Successfully loaded data to BigQuery.')

      if __name__ == '__main__':
          # Read inputs from Kestra
          api_url = """{{ inputs.api_path_and_query }}"""
          headers = {"Authorization": "{{ kv('INFOR_API_TOKEN') }}", "X-Infor-MongooseConfig": '"MITRAPRODIN_PRD_MITRAPRO"'}
          strategy = "{{ inputs.append_strategy }}"
          start_date_str = "{{ inputs.start_date }}"
          site_field = "{{ inputs.site_field }}"
          site = "{{ inputs.site }}"
          date_field = "{{ inputs.date_field }}"
          project_id = "{{ kv('GCP_PROJECT_ID') }}"
          destination_table = "{{ inputs.bq_destination_table }}"
          schema_dict = ast.literal_eval("""{{ inputs.bq_schema_fields }}""")

          with open('sa_key.json', 'r') as f:
              sa_info = json.load(f)
          
          # Create the credentials object
          credentials = service_account.Credentials.from_service_account_info(sa_info)
          
          # Run the two phases
          extracted_df = extract_and_transform(api_url, headers, strategy, start_date_str, schema_dict, date_field, site, site_field)
          load_to_bigquery(extracted_df, project_id, destination_table, credentials)

outputs:
  - id: total_rows
    type: STRING
    value: "{{ outputs.extract_and_load.vars.total_data }}"
  - id: start_date
    type: STRING
    value: "{{ outputs.extract_and_load.vars.start_date }}"
  - id: end_date
    type: STRING
    value: "{{ outputs.extract_and_load.vars.end_date }}"
  - id: year
    type: STRING
    value: "{{ outputs.extract_and_load.vars.year }}"
  - id: month
    type: STRING
    value: "{{ outputs.extract_and_load.vars.month }}"